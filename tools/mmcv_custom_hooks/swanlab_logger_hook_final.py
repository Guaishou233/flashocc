from mmcv.runner import HOOKS, LoggerHook
import swanlab
import torch
import time
from tqdm import tqdm

@HOOKS.register_module()
class SwanLabLoggerHook(LoggerHook):
    def __init__(self, project="mmdet3d", run_name=None, interval=50, 
                 enable_progress_bar=True):
        super().__init__(interval=interval)
        self.project = project
        self.run_name = run_name  # ç›´æ¥ä»é…ç½®æ–‡ä»¶è¯»å–
        self._swanlab_initialized = False
        self.enable_progress_bar = enable_progress_bar
        self.start_time = None
        self.epoch_start_time = None
        self.total_pbar = None  # æ€»è®­ç»ƒè¿›åº¦æ¡
        self.epoch_pbar = None  # å½“å‰epochè¿›åº¦æ¡
        self.validation_losses = []
        self.best_val_loss = float('inf')
        self.overfitting_patience = 3  # è¿ç»­3ä¸ªepochéªŒè¯æŸå¤±ä¸ä¸‹é™åˆ™è®¤ä¸ºè¿‡æ‹Ÿåˆ
        self.overfitting_counter = 0
        self.total_iters = 0  # æ€»è¿­ä»£æ•°
        self.current_iter = 0  # å½“å‰è¿­ä»£æ•°

    def _init_swanlab(self, runner):
        """åˆå§‹åŒ–SwanLabï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„run_name"""
        if self._swanlab_initialized:
            return
        
        # ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­ä¼ å…¥çš„run_name
        swanlab.init(project=self.project, run_name=self.run_name)
        self._swanlab_initialized = True
        if self.run_name:
            print(f"ğŸš€ SwanLab initialized with run_name: {self.run_name}")

    def before_run(self, runner):
        """è®­ç»ƒå¼€å§‹å‰åˆå§‹åŒ–SwanLab"""
        self._init_swanlab(runner)

    def log(self, runner):
        # ç¡®ä¿SwanLabå·²åˆå§‹åŒ–
        if not self._swanlab_initialized:
            self._init_swanlab(runner)
        
        # è·å–log_bufferä¸­çš„æ‰€æœ‰è¾“å‡º
        log_dict = runner.log_buffer.output
        
        # è®°å½•æ‰€æœ‰æ•°å€¼ç±»å‹çš„æŒ‡æ ‡
        metrics = {}
        for k, v in log_dict.items():
            if isinstance(v, (int, float)):
                metrics[k] = v
            elif isinstance(v, torch.Tensor) and v.numel() == 1:
                # å¤„ç†å•ä¸ªå…ƒç´ çš„tensor
                metrics[k] = v.item()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯æŒ‡æ ‡
        val_indicators = [k for k in metrics.keys() if 'val' in k.lower()]
        if val_indicators:
            # å¤„ç†éªŒè¯ç»“æœ
            self._process_validation_results(runner, metrics)
        
        # å¦‚æœæœ‰æŒ‡æ ‡éœ€è¦è®°å½•ï¼Œåˆ™è®°å½•åˆ°SwanLab
        if metrics:
            swanlab.log(metrics, step=runner.iter)
        
        # é¢å¤–è®°å½•ä¸€äº›è®­ç»ƒçŠ¶æ€ä¿¡æ¯
        if hasattr(runner, 'epoch'):
            swanlab.log({'epoch': runner.epoch}, step=runner.iter)
        
        # è®°å½•å­¦ä¹ ç‡
        if hasattr(runner, 'optimizer') and runner.optimizer is not None:
            for i, param_group in enumerate(runner.optimizer.param_groups):
                if 'lr' in param_group:
                    swanlab.log({f'lr_group_{i}': param_group['lr']}, step=runner.iter)

    def before_train_epoch(self, runner):
        """è®­ç»ƒepochå¼€å§‹å‰åˆå§‹åŒ–è¿›åº¦æ¡"""
        # ç¡®ä¿SwanLabå·²åˆå§‹åŒ–
        if not self._swanlab_initialized:
            self._init_swanlab(runner)
        
        if self.enable_progress_bar:
            if self.start_time is None:
                self.start_time = time.time()
                # åˆå§‹åŒ–æ€»è¿›åº¦æ¡
                self.total_iters = len(runner.data_loader) * runner.max_epochs
                self.total_pbar = tqdm(
                    total=self.total_iters,
                    desc='æ€»è®­ç»ƒè¿›åº¦',
                    unit='iter',
                    ncols=120,
                    position=0,
                    leave=True
                )
            
            self.epoch_start_time = time.time()
            
            # è®¡ç®—å½“å‰epochçš„è¿­ä»£æ•°
            epoch_iters = len(runner.data_loader)
            self.epoch_pbar = tqdm(
                total=epoch_iters,
                desc=f'Epoch {runner.epoch + 1}/{runner.max_epochs}',
                unit='iter',
                ncols=120,
                position=1,
                leave=False
            )

    def after_train_epoch(self, runner):
        """è®­ç»ƒepochç»“æŸåæ›´æ–°è¿›åº¦æ¡å’Œè®°å½•æŒ‡æ ‡"""
        if self.epoch_pbar:
            self.epoch_pbar.close()
            self.epoch_pbar = None
        
        # è®¡ç®—epochæ—¶é—´
        if self.epoch_start_time is not None:
            epoch_time = time.time() - self.epoch_start_time
        else:
            epoch_time = 0
        
        if self.start_time is not None:
            total_time = time.time() - self.start_time
        else:
            total_time = 0
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´
        if runner.epoch > 0:
            avg_epoch_time = total_time / (runner.epoch + 1)
            remaining_epochs = runner.max_epochs - (runner.epoch + 1)
            estimated_remaining_time = avg_epoch_time * remaining_epochs
        else:
            estimated_remaining_time = 0
        
        # è®°å½•æ—¶é—´ä¿¡æ¯åˆ°SwanLab
        time_metrics = {
            'epoch_time': epoch_time,
            'total_time': total_time,
            'estimated_remaining_time': estimated_remaining_time
        }
        swanlab.log(time_metrics, step=runner.epoch)
        
        # ç®€åŒ–è°ƒè¯•è¾“å‡º
        print(f"\nğŸ“‹ Epoch {runner.epoch + 1} è®­ç»ƒå®Œæˆ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç»“æœåœ¨log_bufferä¸­
        if hasattr(runner, 'log_buffer') and runner.log_buffer.output:
            log_dict = runner.log_buffer.output
            val_indicators = [k for k in log_dict.keys() if 'val' in k.lower()]
            if val_indicators:
                # ç›´æ¥å¤„ç†éªŒè¯ç»“æœ
                self._process_validation_results(runner, log_dict)

    def _process_validation_results(self, runner, log_dict):
        """å¤„ç†éªŒè¯ç»“æœçš„è¾…åŠ©æ–¹æ³•"""
        # æå–éªŒè¯æŸå¤±
        val_loss = 0
        val_metrics = {}
        
        # æŸ¥æ‰¾éªŒè¯ç›¸å…³çš„æŒ‡æ ‡
        for key, value in log_dict.items():
            if 'val' in key.lower() or 'loss' in key.lower():
                if isinstance(value, (int, float)):
                    val_metrics[key] = value
                    if 'loss' in key.lower() and val_loss == 0:
                        val_loss = value
                elif isinstance(value, torch.Tensor) and value.numel() == 1:
                    val_metrics[key] = value.item()
                    if 'loss' in key.lower() and val_loss == 0:
                        val_loss = value.item()
        
        # å¦‚æœæ²¡æ‰¾åˆ°éªŒè¯æŸå¤±ï¼Œå°è¯•å…¶ä»–å¸¸è§çš„æŸå¤±åç§°
        if val_loss == 0:
            for key, value in log_dict.items():
                if any(loss_name in key.lower() for loss_name in ['loss', 'ce_loss', 'crossentropy', 'focal_loss']):
                    if isinstance(value, (int, float)) and value > 0:
                        val_loss = value
                        val_metrics[f'val_{key}'] = value
                        break
                    elif isinstance(value, torch.Tensor) and value.numel() == 1 and value.item() > 0:
                        val_loss = value.item()
                        val_metrics[f'val_{key}'] = value.item()
                        break
        
        # å¦‚æœæ‰¾åˆ°äº†éªŒè¯æŸå¤±ï¼Œè¿›è¡Œè¿‡æ‹Ÿåˆæ£€æµ‹
        if val_loss > 0:
            self.validation_losses.append(val_loss)
            
            # è®°å½•éªŒè¯æŸå¤±åˆ°SwanLab
            val_metrics.update({
                'val_loss': val_loss,
                'val_loss_avg': sum(self.validation_losses) / len(self.validation_losses)
            })
            
            # æ£€æŸ¥è¿‡æ‹Ÿåˆ
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.overfitting_counter = 0
            else:
                self.overfitting_counter += 1
            
            # è®°å½•è¿‡æ‹ŸåˆçŠ¶æ€
            overfitting_status = {
                'overfitting_counter': self.overfitting_counter,
                'is_overfitting': self.overfitting_counter >= self.overfitting_patience
            }
            val_metrics.update(overfitting_status)
            
            if self.overfitting_counter >= self.overfitting_patience:
                print(f"    âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å¯èƒ½çš„è¿‡æ‹Ÿåˆ! éªŒè¯æŸå¤±å·²è¿ç»­{self.overfitting_counter}ä¸ªepochæœªä¸‹é™")
            
            print(f"    âœ… éªŒè¯æŸå¤±: {val_loss:.4f}")
        else:
            # å³ä½¿æ²¡æœ‰æ‰¾åˆ°éªŒè¯æŸå¤±ï¼Œä¹Ÿè®°å½•ä¸€äº›åŸºæœ¬ä¿¡æ¯
            val_metrics = {'val_loss': 0, 'validation_error': 1}
        
        # è®°å½•æ‰€æœ‰éªŒè¯æŒ‡æ ‡åˆ°SwanLab
        if val_metrics:
            print(f"    ğŸ“ è®°å½•åˆ°SwanLab: {val_metrics}")
            swanlab.log(val_metrics, step=runner.epoch)

    def before_val_epoch(self, runner):
        """éªŒè¯epochå¼€å§‹å‰"""
        print(f"\nğŸš€ å¼€å§‹éªŒè¯ - Epoch {runner.epoch + 1}")

    def after_val_iter(self, runner):
        """éªŒè¯è¿­ä»£å"""
        # å¯ä»¥åœ¨è¿™é‡Œè®°å½•æ¯ä¸ªéªŒè¯batchçš„æŸå¤±
        if hasattr(runner, 'outputs') and runner.outputs is not None:
            outputs = runner.outputs
            if isinstance(outputs, dict):
                metrics = {}
                for k, v in outputs.items():
                    if isinstance(v, (int, float)):
                        metrics[k] = v
                    elif isinstance(v, torch.Tensor) and v.numel() == 1:
                        metrics[k] = v.item()
                
                if metrics:
                    # è®°å½•éªŒè¯è¿­ä»£æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
                    val_iter_metrics = {f'val_iter_{k}': v for k, v in metrics.items()}
                    swanlab.log(val_iter_metrics, step=runner.iter)

    def after_val_epoch(self, runner):
        """éªŒè¯epochç»“æŸåè®°å½•éªŒè¯ç»“æœ"""
        print(f"\nğŸ” éªŒè¯å®Œæˆ - Epoch {runner.epoch + 1}")
        
        # ä»log_bufferè·å–éªŒè¯ç»“æœ
        if hasattr(runner, 'log_buffer') and runner.log_buffer.output:
            log_dict = runner.log_buffer.output
            
            # æå–éªŒè¯æŸå¤±
            val_loss = 0
            val_metrics = {}
            
            # æŸ¥æ‰¾éªŒè¯ç›¸å…³çš„æŒ‡æ ‡
            for key, value in log_dict.items():
                if 'val' in key.lower() or 'loss' in key.lower():
                    if isinstance(value, (int, float)):
                        val_metrics[key] = value
                        if 'loss' in key.lower() and val_loss == 0:
                            val_loss = value
                    elif isinstance(value, torch.Tensor) and value.numel() == 1:
                        val_metrics[key] = value.item()
                        if 'loss' in key.lower() and val_loss == 0:
                            val_loss = value.item()
            
            # å¦‚æœæ²¡æ‰¾åˆ°éªŒè¯æŸå¤±ï¼Œå°è¯•å…¶ä»–å¸¸è§çš„æŸå¤±åç§°
            if val_loss == 0:
                for key, value in log_dict.items():
                    if any(loss_name in key.lower() for loss_name in ['loss', 'ce_loss', 'crossentropy', 'focal_loss']):
                        if isinstance(value, (int, float)) and value > 0:
                            val_loss = value
                            val_metrics[f'val_{key}'] = value
                            break
                        elif isinstance(value, torch.Tensor) and value.numel() == 1 and value.item() > 0:
                            val_loss = value.item()
                            val_metrics[f'val_{key}'] = value.item()
                            break
            
            # å¦‚æœæ‰¾åˆ°äº†éªŒè¯æŸå¤±ï¼Œè¿›è¡Œè¿‡æ‹Ÿåˆæ£€æµ‹
            if val_loss > 0:
                self.validation_losses.append(val_loss)
                
                # è®°å½•éªŒè¯æŸå¤±åˆ°SwanLab
                val_metrics.update({
                    'val_loss': val_loss,
                    'val_loss_avg': sum(self.validation_losses) / len(self.validation_losses)
                })
                
                # æ£€æŸ¥è¿‡æ‹Ÿåˆ
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.overfitting_counter = 0
                else:
                    self.overfitting_counter += 1
                
                # è®°å½•è¿‡æ‹ŸåˆçŠ¶æ€
                overfitting_status = {
                    'overfitting_counter': self.overfitting_counter,
                    'is_overfitting': self.overfitting_counter >= self.overfitting_patience
                }
                val_metrics.update(overfitting_status)
                
                if self.overfitting_counter >= self.overfitting_patience:
                    print(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å¯èƒ½çš„è¿‡æ‹Ÿåˆ! éªŒè¯æŸå¤±å·²è¿ç»­{self.overfitting_counter}ä¸ªepochæœªä¸‹é™")
                
                print(f"âœ… éªŒè¯æŸå¤±: {val_loss:.4f}")
            else:
                # å³ä½¿æ²¡æœ‰æ‰¾åˆ°éªŒè¯æŸå¤±ï¼Œä¹Ÿè®°å½•ä¸€äº›åŸºæœ¬ä¿¡æ¯
                val_metrics = {'val_loss': 0, 'validation_status': 'no_loss_found'}
            
            # è®°å½•æ‰€æœ‰éªŒè¯æŒ‡æ ‡åˆ°SwanLab
            if val_metrics:
                print(f"ğŸ“ è®°å½•åˆ°SwanLab: {val_metrics}")
                swanlab.log(val_metrics, step=runner.epoch)
        else:
            # è®°å½•éªŒè¯å¤±è´¥çŠ¶æ€
            swanlab.log({'val_loss': 0, 'validation_error': 2}, step=runner.epoch)

    def after_train_iter(self, runner):
        """åœ¨è®­ç»ƒè¿­ä»£åè®°å½•losså€¼å’Œæ›´æ–°è¿›åº¦æ¡"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        super().after_train_iter(runner)
        
        # æ›´æ–°å½“å‰è¿­ä»£è®¡æ•°
        self.current_iter = runner.iter
        
        # æ›´æ–°æ€»è¿›åº¦æ¡
        if self.total_pbar:
            self.total_pbar.update(1)
            
            # è®¡ç®—æ€»è¿›åº¦ç™¾åˆ†æ¯”
            total_progress = (self.current_iter / self.total_iters) * 100
            
            # è®¡ç®—æ€»é¢„è®¡å‰©ä½™æ—¶é—´
            if self.current_iter > 0 and self.start_time:
                elapsed = time.time() - self.start_time
                avg_time_per_iter = elapsed / self.current_iter
                remaining_iters = self.total_iters - self.current_iter
                total_eta = avg_time_per_iter * remaining_iters
                
                # æ›´æ–°æ€»è¿›åº¦æ¡æ˜¾ç¤º
                self.total_pbar.set_postfix({
                    'æ€»è¿›åº¦': f'{total_progress:.1f}%',
                    'æ€»ETA': f'{total_eta/60:.1f}min'
                })
            else:
                # å¦‚æœæ— æ³•è®¡ç®—ETAï¼Œåªæ˜¾ç¤ºè¿›åº¦
                self.total_pbar.set_postfix({
                    'æ€»è¿›åº¦': f'{total_progress:.1f}%'
                })
        
        # æ›´æ–°epochè¿›åº¦æ¡
        if self.epoch_pbar:
            self.epoch_pbar.update(1)
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºepoché¢„è®¡å‰©ä½™æ—¶é—´
            if runner.iter > 0 and self.epoch_start_time:
                elapsed = time.time() - self.epoch_start_time
                # ä¿®å¤é™¤é›¶é”™è¯¯ï¼šç¡®ä¿åˆ†æ¯ä¸ä¸º0
                current_epoch_iter = runner.iter % len(runner.data_loader)
                if current_epoch_iter > 0:
                    avg_time_per_iter = elapsed / current_epoch_iter
                    remaining_iters = len(runner.data_loader) - current_epoch_iter
                    epoch_eta = avg_time_per_iter * remaining_iters
                    self.epoch_pbar.set_postfix({'ETA': f'{epoch_eta:.1f}s'})
                else:
                    self.epoch_pbar.set_postfix({'ETA': 'è®¡ç®—ä¸­...'})
        
        # ä»runner.outputsè·å–losså€¼ï¼ˆMMCVçš„æ–¹å¼ï¼‰
        if hasattr(runner, 'outputs') and runner.outputs is not None:
            outputs = runner.outputs
            if isinstance(outputs, dict):
                # å¤„ç†MMCVçš„train_stepè¿”å›çš„log_vars
                metrics = {}
                for k, v in outputs.items():
                    if isinstance(v, (int, float)):
                        metrics[k] = v
                    elif isinstance(v, torch.Tensor) and v.numel() == 1:
                        # å¤„ç†å•ä¸ªå…ƒç´ çš„tensor
                        metrics[k] = v.item()
                    elif isinstance(v, torch.Tensor):
                        # å¤„ç†å¤šå…ƒç´ tensorï¼Œå–å¹³å‡å€¼
                        metrics[k] = v.mean().item()
                
                # è®°å½•losså€¼åˆ°SwanLab
                if metrics:
                    swanlab.log(metrics, step=runner.iter)

    def after_train(self, runner):
        """è®­ç»ƒç»“æŸåæ¸…ç†è¿›åº¦æ¡"""
        if self.total_pbar:
            self.total_pbar.close()
            self.total_pbar = None
        if self.epoch_pbar:
            self.epoch_pbar.close()
            self.epoch_pbar = None
